{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Clone the NanoGPT\n!git clone https://github.com/karpathy/nanoGPT.git\n!pip install tiktoken\n!pip install wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T11:18:51.183597Z","iopub.execute_input":"2024-12-07T11:18:51.183944Z","iopub.status.idle":"2024-12-07T11:19:10.217760Z","shell.execute_reply.started":"2024-12-07T11:18:51.183912Z","shell.execute_reply":"2024-12-07T11:19:10.216871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom contextlib import nullcontext\nimport nanoGPT.model as GPT\nimport wandb\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T11:19:10.219348Z","iopub.execute_input":"2024-12-07T11:19:10.219611Z","iopub.status.idle":"2024-12-07T11:19:12.667733Z","shell.execute_reply.started":"2024-12-07T11:19:10.219585Z","shell.execute_reply":"2024-12-07T11:19:12.666830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Directly set your API key (not secure in public notebooks)\nimport wandb\nwandb.login(key=\"2c31d7e5323a64ac198ab2499a802513a1ac5ec8\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T11:19:12.668937Z","iopub.execute_input":"2024-12-07T11:19:12.669298Z","iopub.status.idle":"2024-12-07T11:19:13.746700Z","shell.execute_reply.started":"2024-12-07T11:19:12.669260Z","shell.execute_reply":"2024-12-07T11:19:13.746020Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GPTConfig: # Model config from NanoGPT\n    block_size: int = 1024\n    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    # dropout: float = 0.0\n    dropout: float = 0.2 \n    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n\nconfig = GPTConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T11:19:13.748236Z","iopub.execute_input":"2024-12-07T11:19:13.748602Z","iopub.status.idle":"2024-12-07T11:19:13.752980Z","shell.execute_reply.started":"2024-12-07T11:19:13.748575Z","shell.execute_reply":"2024-12-07T11:19:13.752119Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install datasets\n\nfrom datasets import load_dataset\n\nds = load_dataset(\"roneneldan/TinyStories\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T11:19:13.754094Z","iopub.execute_input":"2024-12-07T11:19:13.754343Z","iopub.status.idle":"2024-12-07T11:19:36.632296Z","shell.execute_reply.started":"2024-12-07T11:19:13.754318Z","shell.execute_reply":"2024-12-07T11:19:36.631379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tiktoken\nimport os\n\nenc = tiktoken.get_encoding(\"gpt2\")\n\n# Some functions from https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/prepare.py\n\ndef process(example):\n    ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n    out = {'ids': ids, 'len': len(ids)}\n    return out\n\nif not os.path.exists(\"train.bin\"):\n    tokenized = ds.map(\n        process,\n        remove_columns=['text'],\n        desc=\"tokenizing the splits\",\n        num_proc=8,\n        )\n    # concatenate all the ids in each dataset into one large file we can use for training\n    for split, dset in tokenized.items():\n        arr_len = np.sum(dset['len'], dtype=np.uint64)\n        filename = f'{split}.bin'\n        dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n        total_batches = 1024\n\n        idx = 0\n        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n            # Batch together samples for faster write\n            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n            arr_batch = np.concatenate(batch['ids'])\n            # Write into mmap\n            arr[idx : idx + len(arr_batch)] = arr_batch\n            idx += len(arr_batch)\n        arr.flush()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T11:19:36.633376Z","iopub.execute_input":"2024-12-07T11:19:36.633805Z","iopub.status.idle":"2024-12-07T11:22:48.512660Z","shell.execute_reply.started":"2024-12-07T11:19:36.633777Z","shell.execute_reply":"2024-12-07T11:22:48.511668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Some functions from https://github.com/karpathy/nanoGPT/blob/master/train.py with slight modifications\ndef get_batch(split):\n    # We recreate np.memmap every batch to avoid a memory leak, as per\n    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n    if split == 'train':\n        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n    else:\n        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    if device_type == 'cuda':\n        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n    else:\n        x, y = x.to(device), y.to(device)\n    return x, y\n\n\ndef estimate_loss(model):\n    out = {}\n    model.eval()\n    with torch.inference_mode():\n        for split in ['train', 'val']:\n            losses = torch.zeros(eval_iters)\n            for k in range(eval_iters):\n                X, Y = get_batch(split)\n                with ctx:\n                    logits, loss = model(X, Y)\n                losses[k] = loss.item()\n            out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T11:22:48.513908Z","iopub.execute_input":"2024-12-07T11:22:48.514210Z","iopub.status.idle":"2024-12-07T11:22:48.522411Z","shell.execute_reply.started":"2024-12-07T11:22:48.514183Z","shell.execute_reply":"2024-12-07T11:22:48.521444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training Config\n\nlearning_rate = 5e-4  # Reduced learning rate\nmax_iters = 5000      # Slightly reduced iterations\nwarmup_steps = 200    # Increased warmup\nmin_lr = 1e-5         # Adjusted minimum learning rate\neval_iters = 100\nbatch_size = 6        # Slight increase in batch size\nblock_size = 1024\ngradient_accumulation_steps = 24  # Reduced accumulation steps\n\ndevice =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n# note: float16 data type will automatically use a GradScaler\n\n# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n\nctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n\ntorch.set_default_device(device)\ntorch.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T11:22:48.523512Z","iopub.execute_input":"2024-12-07T11:22:48.523766Z","iopub.status.idle":"2024-12-07T11:22:48.775636Z","shell.execute_reply.started":"2024-12-07T11:22:48.523741Z","shell.execute_reply":"2024-12-07T11:22:48.774680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n\nnanoGPT = GPT.GPT(config)\n# optimizer =  torch.optim.AdamW(nanoGPT.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\noptimizer = torch.optim.AdamW(\n    nanoGPT.parameters(), \n    lr=learning_rate, \n    betas=(0.9, 0.95),  # Adjusted betas\n    weight_decay=0.01    # Added light weight decay\n)\n\nscheduler_warmup = LinearLR(optimizer, total_iters=warmup_steps)\nscheduler_decay = CosineAnnealingLR(\n    optimizer, \n    T_max=max_iters - warmup_steps, \n    eta_min=min_lr\n)\nscheduler = SequentialLR(\n    optimizer, \n    schedulers=[scheduler_warmup, scheduler_decay], \n    milestones=[warmup_steps]\n)\n# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T11:22:48.776747Z","iopub.execute_input":"2024-12-07T11:22:48.777051Z","iopub.status.idle":"2024-12-07T11:22:49.523669Z","shell.execute_reply.started":"2024-12-07T11:22:48.777013Z","shell.execute_reply":"2024-12-07T11:22:49.522703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.init(\n    project=\"LLM-Training-Assignment\",  # Choose a meaningful project name\n    config={\n        \"learning_rate\": learning_rate,\n        \"batch_size\": batch_size,\n        \"max_iters\": max_iters,\n        \"model_layers\": config.n_layer,\n        \"model_heads\": config.n_head,\n        \"model_embedding_dim\": config.n_embd\n    }\n)\nbest_val_loss = float('inf')\nbest_model_params_path = \"best_model_params.pt\"\ntrain_loss_list, validation_loss_list = [], []\n\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# Initialize variables for early stopping\nearly_stop_count = 0\npatience = 8  # Number of evaluation intervals to wait before stopping\nbest_val_loss = float('inf')\n\n# Set up ReduceLROnPlateau scheduler\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n\ntry:\n    for epoch in tqdm(range(max_iters)):\n        # Evaluation and Logging\n        if epoch % eval_iters == 0 and epoch != 0:\n            losses = estimate_loss(nanoGPT)\n            train_loss = losses['train']\n            val_loss = losses['val']\n            \n            print(f\"Epoch {epoch}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n            print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n            \n            train_loss_list.append(train_loss)\n            validation_loss_list.append(val_loss)\n            \n            wandb.log({\n                \"epoch\": epoch,\n                \"train/loss\": train_loss,\n                \"val/loss\": val_loss,\n                \"lr\": optimizer.param_groups[0]['lr']\n            })\n            \n            # Save the best model\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                early_stop_count = 0  # Reset patience counter\n                torch.save(nanoGPT.state_dict(), best_model_params_path)\n                print(f\"New best model saved with val loss: {val_loss:.4f}\")\n            else:\n                early_stop_count += 1\n                print(f\"No improvement. Early stop count: {early_stop_count}/{patience}\")\n            \n            # Early stopping condition\n            if early_stop_count >= patience:\n                print(\"Early stopping triggered. Training terminated.\")\n                break\n            \n            # Update learning rate based on validation loss\n            scheduler.step(val_loss)\n\n        # Training Step\n        X, y = get_batch(\"train\")\n        with ctx:  # Mixed precision context\n            logits, loss = nanoGPT(X, y)\n            loss = loss / gradient_accumulation_steps\n            scaler.scale(loss).backward()\n\n        # Update weights and learning rate at accumulation step\n        if (epoch + 1) % gradient_accumulation_steps == 0 or (epoch + 1 == max_iters):\n            # Gradient Clipping\n            torch.nn.utils.clip_grad_norm_(nanoGPT.parameters(), 1.0)\n            \n            # Optimizer step\n            scaler.step(optimizer)\n            optimizer.zero_grad(set_to_none=True)\n            scaler.update()\n\nfinally:\n    wandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T11:22:49.526779Z","iopub.execute_input":"2024-12-07T11:22:49.527296Z","iopub.status.idle":"2024-12-07T13:38:42.453065Z","shell.execute_reply.started":"2024-12-07T11:22:49.527267Z","shell.execute_reply":"2024-12-07T13:38:42.452294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ntrain_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\nvalidation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\nplt.plot(train_loss_list_converted, 'g', validation_loss_list_converted, 'r')\nplt.xlabel(\"Steps - Every 100 epochs\")\nplt.ylabel(\"Loss\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:38:42.454188Z","iopub.execute_input":"2024-12-07T13:38:42.454466Z","iopub.status.idle":"2024-12-07T13:38:42.712287Z","shell.execute_reply.started":"2024-12-07T13:38:42.454438Z","shell.execute_reply":"2024-12-07T13:38:42.711300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Load the model\nnanoGPT = GPT.GPT(config)\ndevice =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\nbest_model_params_path = \"best_model_params.pt\"\nnanoGPT.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:38:42.713636Z","iopub.execute_input":"2024-12-07T13:38:42.713931Z","iopub.status.idle":"2024-12-07T13:38:43.096047Z","shell.execute_reply.started":"2024-12-07T13:38:42.713902Z","shell.execute_reply":"2024-12-07T13:38:43.095046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentence = \"There was a\"\ncontext = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\ny = nanoGPT.generate(context, 200)\nprint(enc.decode(y.squeeze().tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T13:38:43.097320Z","iopub.execute_input":"2024-12-07T13:38:43.097702Z","iopub.status.idle":"2024-12-07T13:38:45.240463Z","shell.execute_reply.started":"2024-12-07T13:38:43.097660Z","shell.execute_reply":"2024-12-07T13:38:45.239452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}